{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pydantic import BaseSettings\n",
    "from matplotlib import  pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import iqr\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsConfig(BaseSettings):\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        env_file_encoding = \"utf-8\"\n",
    "    data_processed: Path\n",
    "    subset: str = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path(os.getcwd())\n",
    "os.chdir(str(cwd.parent))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = StatsConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from floods.datasets import flood\n",
    "from floods import prepare\n",
    "\n",
    "importlib.reload(flood)\n",
    "importlib.reload(prepare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = flood.FloodDataset(path=cfg.data_processed, subset=\"train\", include_dem=True)\n",
    "loader = DataLoader(dataset, batch_size=64, num_workers=4, pin_memory=True, shuffle=False)\n",
    "compute_stats = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Measure the percentile intervals and then estimate Standard Deviation of the distribution, both from median to the 90th percentile and from the 10th to 90th percentile\n",
    "if compute_stats:\n",
    "    p25s = list()\n",
    "    p75s = list()\n",
    "    p50s = list()\n",
    "    minval = np.ones(3) * np.finfo(np.float32).max\n",
    "    maxval = np.ones(3) * np.finfo(np.float32).min\n",
    "\n",
    "\n",
    "    # compute a robust standard deviation using 10th and 90th percentile\n",
    "    for image, label in tqdm(loader):\n",
    "        image = image.numpy().reshape(-1, 3)\n",
    "        valid = label.flatten() != 255\n",
    "        image = image[valid]\n",
    "        minval = np.minimum(minval, np.min(image, axis=0))\n",
    "        maxval = np.maximum(maxval, np.max(image, axis=0))\n",
    "\n",
    "        p75, p25 = np.percentile(image, (75, 25), axis=0)\n",
    "        p25s.append(p25)\n",
    "        p75s.append(p75)\n",
    "        p50s.append(np.median(image, axis=0))\n",
    "\n",
    "    p25 = np.stack(p25s).mean(axis=0)\n",
    "    p75 = np.stack(p75s).mean(axis=0)\n",
    "    p50 = np.stack(p50s).mean(axis=0)\n",
    "\n",
    "    iqr = p75 - p25\n",
    "    sigma = iqr / 1.34896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p50 = np.array([4.9329374e-02, 1.1776519e-02, 1.4241237e+02])\n",
    "sigma = np.array([3.91287043e-02, 1.03687926e-02, 8.11010422e+01])\n",
    "print(f\"std: {sigma}\")\n",
    "print(f\"median: {p50}\")\n",
    "# print(f\"min: {[f'{v:.4f}' for v in minval]}\")\n",
    "# print(f\"max: {[f'{v:.4f}' for v in maxval]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_stats:\n",
    "    factor = 2\n",
    "\n",
    "    clip_min = p50 - factor * sigma\n",
    "    clip_max = p50 + factor * sigma\n",
    "    # store values\n",
    "    means = list()\n",
    "    stds = list()\n",
    "    # compute robust mean and std on data outside (factor x) iqr\n",
    "    for image, label in tqdm(loader):\n",
    "        image = image.reshape(-1, 3)\n",
    "        valid = label.flatten() != 255\n",
    "        image = image[valid]\n",
    "\n",
    "        image = np.clip(image, clip_min, clip_max)\n",
    "        means.append(image.mean(axis=0))\n",
    "        stds.append(image.std(axis=0))\n",
    "\n",
    "    means = np.stack(means).mean(axis=0)\n",
    "    stds = np.stack(stds).mean(axis=0)\n",
    "    print(f\"avg: {means}\")\n",
    "    print(f\"std: {stds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 10\n",
    "# clip_min = p50 - factor * sigma\n",
    "# clip_max = p50 + factor * sigma\n",
    "# print(clip_max, clip_min)\n",
    "clip_min = np.array([-50.0, -50.0, -50.0])\n",
    "clip_max = np.array([+50, 1, 6000.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipNormalize(alb.Normalize):\n",
    "\n",
    "    def __init__(self,\n",
    "                 mean: tuple,\n",
    "                 std: tuple,\n",
    "                 clip_min: tuple,\n",
    "                 clip_max: tuple,\n",
    "                 max_pixel_value: float = 1.0,\n",
    "                 always_apply: bool = False,\n",
    "                 p: float = 1.0):\n",
    "        super().__init__(mean=mean, std=std, max_pixel_value=max_pixel_value, always_apply=always_apply, p=p)\n",
    "        self.min = clip_min\n",
    "        self.max = clip_max\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        result = super().apply(image=image, **params)\n",
    "        return np.clip(result, self.min, self.max)\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        parent = list(super().get_transform_init_args_names())\n",
    "        return tuple(parent + [\"clip_min\", \"clip_max\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_trf = alb.Compose([ClipNormalize(mean=p50, std=sigma, clip_min=-30, clip_max=30),\n",
    "                        ToTensorV2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cls = flood.FloodDataset\n",
    "dataset2 = dataset_cls(path=cfg.data_processed, subset=\"train\", include_dem=True, transform_base=base_trf)\n",
    "loader2 = DataLoader(dataset2, batch_size=32, num_workers=4, pin_memory=True, shuffle=True)\n",
    "plot_loader = loader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# compute robust mean and std on data outside (factor x) iqr\n",
    "for i, (batch, label) in tqdm(enumerate(plot_loader)):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    batch = batch.numpy()\n",
    "    batch = np.swapaxes(batch, 0, 1)\n",
    "    print(batch.shape)\n",
    "\n",
    "    sns.histplot(batch[0].flatten(), bins=500)\n",
    "    plt.title(\"vv\")\n",
    "    plt.show()\n",
    "    sns.histplot(batch[1].flatten(), bins=500)\n",
    "    plt.title(\"vh\")\n",
    "    plt.show()\n",
    "    sns.histplot(batch[2].flatten(), bins=500)\n",
    "    plt.title(\"dem\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = list()\n",
    "# stds = list()\n",
    "# # compute robust mean and std on data outside (factor x) iqr\n",
    "# for image, label in tqdm(loader2):\n",
    "#     image = image.reshape(-1, 3)\n",
    "#     valid = label.flatten() != 255\n",
    "#     image = image[valid]\n",
    "\n",
    "#     means.append(image.mean(axis=0))\n",
    "#     stds.append(image.std(axis=0))\n",
    "\n",
    "# means = np.stack(means).mean(axis=0)\n",
    "# stds = np.stack(stds).mean(axis=0)\n",
    "# print(f\"avg: {means}\")\n",
    "# print(f\"std: {stds}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61bd0401774e7981c2ad7ca0754761d58a145d15f3fee780bc248b1d208f4211"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
